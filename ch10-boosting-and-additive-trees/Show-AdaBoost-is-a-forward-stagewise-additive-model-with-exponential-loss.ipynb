{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation below is adopted from Chapter 10.4 in https://web.stanford.edu/~hastie/Papers/ESLII.pdf, with more details added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponetial loss is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(y, f(x)) = \\exp(-y f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the categories are labeled with $\\{-1, 1\\}$, and there are $N$ training examples.\n",
    "\n",
    "We'll use $x_i$ is the $i$th instance, and $y_i$ is the label of the $i$th instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training at the $m$th step,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At step $m$ of training a forward stagewise additive model:\n",
    "\n",
    "* $f_{m-1}$ is the weighted combination of all base learners learned before step $m$\n",
    "* $\\beta_m$ is the weight of the base learner $G_m$ learned at step $m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to train the $m$th base learner, we want to minimize the following loss,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L_m \n",
    "&= \\sum_{i=1}^N \\exp[- y_i f_m(x_i)] \\\\\n",
    "&= \\sum_{i=1}^N \\exp\\left[- y_i \\big( f_{m - 1}(x_i) + \\beta G(x_i) \\big)\\right]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\beta_m, G_m) = \\arg \\min_{\\beta, G} \\sum_{i=1}^N \\exp \\big(- y_i \\big( f_{m - 1}(x_i) + \\beta G(x_i) \\big) \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With further transformation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(\\beta_m, G_m) \n",
    "&= \\arg \\min_{\\beta, G} \\sum_{i=1}^N \\exp \\big( - y_i f_{m - 1}(x_i) \\big) \\exp \\big( -y_i \\beta G(x_i) \\big) \\\\\n",
    "&= \\arg \\min_{\\beta, G} \\sum_{i=1}^N w_i^{(m)} \\exp \\big( - y_i \\beta G(x_i) \\big) \\\\\n",
    "&= \\arg \\min_{\\beta, G} \\sum_{i=1}^N w_i^{(m)} \\exp \\big( - \\beta y_i  G(x_i) \\big) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w_i^{(m)} = \\exp \\big( - y_i f_{m - 1}(x_i) \\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-hand side can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\sum_{i=1}^N w_i^{(m)} e^{- \\beta y_i  G(x_i)}\n",
    "&= \\sum_{y_i \\neq G(x_i)} w_i^{(m)} e^\\beta + \\sum_{y_i=G(x_i)} w_i^{(m)} e^{-\\beta}   \\\\\n",
    "&= e^\\beta \\sum_{y_i \\neq G(x_i)} w_i^{(m)} + e^{-\\beta} \\sum_{y_i=G(x_i)} w_i^{(m)} \\\\\n",
    "&= e^\\beta \\sum_{y_i \\neq G(x_i)} w_i^{(m)} + \\Big( e^{-\\beta} \\sum_{y_i=G(x_i)} w_i^{(m)} + e^{-\\beta} \\sum_{y_i \\neq G(x_i)} w_i^{(m)} \\Big) - e^{-\\beta} \\sum_{y_i \\neq G(x_i)} w_i^{(m)} \\\\\n",
    "&= e^\\beta \\sum_{y_i \\neq G(x_i)} w_i^{(m)} + e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} - e^{-\\beta} \\sum_{y_i \\neq G(x_i)} w_i^{(m)} \\\\\n",
    "&= (e^\\beta - e^{-\\beta}) \\sum_{y_i \\neq G(x_i)} w_i^{(m)} + e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} \\\\\n",
    "&= (e^\\beta - e^{-\\beta}) \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) + e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the third equality just adds $e^{-\\beta} \\sum_{y_i \\neq G(x_i)} w_i^{(m)}$ and then remove it, so resulting in no change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "G_m\n",
    "&= \\arg \\min_G (e^\\beta - e^{-\\beta}) \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) + e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} \\\\\n",
    "&= \\arg \\min_G \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) \\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the second part of the right-hand side, $e^{-\\beta} \\sum_{i=1}^N w_i^{(m)}$, doesn't depend on $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain $b_m$, we set the derivative of the RHS wrt. $\\beta$ to be equal to 0,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L_m}{\\partial \\beta}\n",
    "&= (e^\\beta + e^{-\\beta}) \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) - e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} = 0 \\\\\n",
    "e^{-\\beta} \\sum_{i=1}^N w_i^{(m)} &= (e^\\beta + e^{-\\beta}) \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) \\\\\n",
    "\\frac{e^\\beta + e^{-\\beta}}{e^{-\\beta}} &= \\frac{\\sum_{i=1}^N w_i^{(m)}}{ \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) } \\\\\n",
    "e^{2\\beta} &= \\frac{\\sum_{i=1}^N w_i^{(m)}}{ \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) } - 1 \\\\\n",
    "\\beta_m &= \\frac{1}{2} \\ln \\left( \\frac{\\sum_{i=1}^N w_i^{(m)}}{ \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) } - 1 \\right) \\\\\n",
    "&= \\frac{1}{2} \\ln \\left(\\frac{1 - \\text{err}_m}{\\text{err}_m} \\right) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{err}_m = \\frac{ \\sum_{i=1}^N w_i^{(m)} \\mathbb{I}(y_i \\neq G(x_i)) }{\\sum_{i=1}^N w_i^{(m)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $G_m$ and $\\beta_m$ calculated,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f_m(x) &= f_{m-1}(x) + \\beta_m G_m(x) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_i^{(m+1)} \n",
    "&= e^{- y_i f_{m}(x_i)} \\\\\n",
    "&= e^{- y_i \\big(f_{m-1}(x_i) + \\beta_m G_m(x_i) \\big)} \\\\\n",
    "&= e^{- y_i f_{m-1}(x_i)} e^{- y_i \\beta_m G_m(x_i)} \\\\\n",
    "&= w_i^{m} e^{- \\beta_m y_i G_m(x_i)} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the fact that $-y_i G_m(x_i) = 2 \\mathbb{I}(y_i \\neq G_m(x_i)) - 1$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_i^{(m+1)}\n",
    "&= w_i^{m} e^{- \\beta_m y_i G_m(x_i)} \\\\\n",
    "&= w_i^{m} e^{\\beta_m (2 \\mathbb{I}(y_i \\neq G_m(x_i)) - 1)} \\\\\n",
    "&= w_i^{m} e^{2\\beta_m \\mathbb{I}(y_i \\neq G_m(x_i)} e^{-\\beta_m} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $e^{-\\beta_m}$ scales the weight for every sample by the same factor, it doesn't have any effect in changing the relative weight among samples, we could ignore it, $w_i^{(m+1)}$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_i^{(m+1)} = w_i^{m} e^{2\\beta_m \\mathbb{I}(y_i \\neq G_m(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting $\\alpha_m = 2\\beta_m = \\ln \\left(\\frac{1 - \\text{err}_m}{\\text{err}_m} \\right)$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_i^{(m+1)}\n",
    "&= w_i^{m} e^{\\alpha_m \\mathbb{I}(y_i \\neq G_m(x_i)}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have shown that updates of the variables, $\\text{err}_m$, $\\alpha_m$, $w_i$ are all equivalent to those listed in Algorithm 10.1 AdaBoost.M1, hence the AdaBoost algorithm is one particular type of forward stagewise additive models with a particular loss function, i.e. exponential loss. Q.E.D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

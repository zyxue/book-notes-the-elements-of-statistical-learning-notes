{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof for Equation (10.16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f^*(x) = \\arg \\min_{f(x)} \\mathbb{E}_{Y|x}\\left[e ^{-Y f(x)}\\right] = \\frac{1}{2}\\frac{\\text{Pr}(Y=1|x)}{\\text{Pr}(Y=-1|x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_{Y|x}\\left[ e ^{-Y f(x)} \\right] \n",
    "&= e ^{- f(x)} \\text{Pr}(Y=1|x) + e ^{f(x)} \\text{Pr}(Y=-1|x) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbb{E}_{Y|x}\\left[ e ^{-Y f(x)} \\right]}{\\partial f(x)}\n",
    "&= 0 \\\\\n",
    "-e ^{-f(x)} \\text{Pr}(Y=1|x) + e ^{f(x)} \\text{Pr}(Y=-1|x)\n",
    "&= 0 \\\\\n",
    "e ^{f(x)} \\text{Pr}(Y=-1|x)\n",
    "&= e ^{-f(x)} \\text{Pr}(Y=1|x) \\\\\n",
    "e ^{2f(x)}\n",
    "&= \\frac{\\text{Pr}(Y=1|x)}{\\text{Pr}(Y=-1|x)} \\\\\n",
    "f(x)\n",
    "&= \\frac{1}{2} \\ln \\frac{\\text{Pr}(Y=1|x)}{\\text{Pr}(Y=-1|x)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which, it's easy to show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{Pr}(Y=1|x) = \\frac{e^{2f(x)}}{1 + e^{2f(x)}} = \\frac{1}{e^{-2f(x)} + 1} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.E.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p = \\text{Pr}(Y=1|x)$. Next, show that minimizing log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "l(Y, p) = Y' \\ln p + (1 - Y') \\ln(1 - p)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is equivalent to minimizing deviance (Equation (10.18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "-d(Y, f(x)) = \\ln (1 + e^{-2Yf(x)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with \n",
    "\n",
    "* $Y' \\in \\{0, 1\\}$\n",
    "* $Y \\in \\{-1, 1\\}$\n",
    "* $ Y = 2Y' - 1$\n",
    "* $f(x) = \\frac{1}{2} \\ln \\frac{p}{1 - p} $, i.e. half logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $Y' = 1$, $Y = 1$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "l(Y, p) &= \\ln p \\\\\n",
    "- d(Y, f(x)) &= - \\ln (1 + e^{-2f(x)}) \\\\\n",
    "&= - \\ln \\left(1 + e^{-2 \\frac{1}{2} \\ln \\frac{p}{1 - p}} \\right) \\\\\n",
    "&= - \\ln \\left(1 + \\frac{1 - p}{p} \\right) \\\\\n",
    "&= \\ln p \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $Y' = 0$, $Y = -1$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "l(Y, p) &= \\ln (1 - p) \\\\\n",
    "- d(Y, f(x)) &= - \\ln (1 + e^{2f(x)}) \\\\\n",
    "&= - \\ln \\left(1 + e^{2 \\frac{1}{2} \\ln \\frac{p}{1 - p}} \\right) \\\\\n",
    "&= - \\ln \\left(1 + \\frac{p}{1 - p} \\right) \\\\\n",
    "&= \\ln (1 - p) \\\\ \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in both cases, $l(Y, p) = - d(Y, f(x))$. Q.E.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, log-loss and deviance are equivalent, the difference is just that the\n",
    "former is defined in terms of probabilities of classes, while the later is\n",
    "defined in terms of logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviance is also called **negative binomial log-likeliklihood** in [Greedy\n",
    "function approximation: A gradient boosting\n",
    "machine](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviance is closely related to the rewriting of logloss in terms of logit as\n",
    "shown in http://zyxue.github.io/2022/11/03/derive-gradient-boosting-using-newtons-method.html#logloss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
